{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa73a39",
   "metadata": {
    "papermill": {
     "duration": 0.006021,
     "end_time": "2023-04-06T04:50:00.027584",
     "exception": false,
     "start_time": "2023-04-06T04:50:00.021563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Steps:\n",
    "\n",
    "- Load data and visualize a sample of them\n",
    "- Check for distribution across classes\n",
    "\n",
    "- Give ERM/IRM a shot to improve the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b89d6d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:00.039124Z",
     "iopub.status.busy": "2023-04-06T04:50:00.038658Z",
     "iopub.status.idle": "2023-04-06T04:50:00.044401Z",
     "shell.execute_reply": "2023-04-06T04:50:00.043359Z"
    },
    "papermill": {
     "duration": 0.014394,
     "end_time": "2023-04-06T04:50:00.046951",
     "exception": false,
     "start_time": "2023-04-06T04:50:00.032557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run following commands if running on local\n",
    "# !pip install kaggle\n",
    "\n",
    "# Download kaggle.json from kaggle website under profile->new API section\n",
    "# !kaggle competitions download -c plant-seedlings-classification\n",
    "# !unzip -q plant-seedlings-classification.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d01b750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:00.058281Z",
     "iopub.status.busy": "2023-04-06T04:50:00.057492Z",
     "iopub.status.idle": "2023-04-06T04:50:00.065692Z",
     "shell.execute_reply": "2023-04-06T04:50:00.064812Z"
    },
    "papermill": {
     "duration": 0.015904,
     "end_time": "2023-04-06T04:50:00.067744",
     "exception": false,
     "start_time": "2023-04-06T04:50:00.051840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# os.chdir('/kaggle/input/plant-seedlings-classification/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63e44cfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:00.079614Z",
     "iopub.status.busy": "2023-04-06T04:50:00.078130Z",
     "iopub.status.idle": "2023-04-06T04:50:11.702799Z",
     "shell.execute_reply": "2023-04-06T04:50:11.701630Z"
    },
    "papermill": {
     "duration": 11.633154,
     "end_time": "2023-04-06T04:50:11.705602",
     "exception": false,
     "start_time": "2023-04-06T04:50:00.072448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224)\n"
     ]
    }
   ],
   "source": [
    "# Folder structure\n",
    "# Training data\n",
    "# contains images in 12 folders, each folder contains images of a single class\n",
    "# Test data\n",
    "# contains all images in a single folder\n",
    "\n",
    "# Load the data\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "size = (\n",
    "    (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "\n",
    "print(size)\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.Resize(size),\n",
    "    # RandomResizedCrop being used here --> https://huggingface.co/docs/transformers/main/en/tasks/image_classification\n",
    "    transforms.RandomRotation(360),\n",
    "    transforms.RandomResizedCrop(size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder('./train', transform=transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f34e07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T00:43:19.982073Z",
     "iopub.status.busy": "2023-04-05T00:43:19.981148Z",
     "iopub.status.idle": "2023-04-05T00:43:19.987338Z",
     "shell.execute_reply": "2023-04-05T00:43:19.985893Z",
     "shell.execute_reply.started": "2023-04-05T00:43:19.982023Z"
    },
    "papermill": {
     "duration": 0.005232,
     "end_time": "2023-04-06T04:50:11.716555",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.711323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08d17656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.729769Z",
     "iopub.status.busy": "2023-04-06T04:50:11.727732Z",
     "iopub.status.idle": "2023-04-06T04:50:11.733964Z",
     "shell.execute_reply": "2023-04-06T04:50:11.733009Z"
    },
    "papermill": {
     "duration": 0.014495,
     "end_time": "2023-04-06T04:50:11.736063",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.721568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot class distribution\n",
    "# from collections import Counter\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# distribution = dict(Counter(dataset.targets))\n",
    "\n",
    "# # Plot class distribution histogram\n",
    "# plt.bar(list(map(lambda x: dataset.classes[x], distribution.keys())), distribution.values())\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144b7050",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-05T00:43:46.338478Z",
     "iopub.status.busy": "2023-04-05T00:43:46.337206Z",
     "iopub.status.idle": "2023-04-05T00:43:46.342784Z",
     "shell.execute_reply": "2023-04-05T00:43:46.341565Z",
     "shell.execute_reply.started": "2023-04-05T00:43:46.338426Z"
    },
    "papermill": {
     "duration": 0.004893,
     "end_time": "2023-04-06T04:50:11.745909",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.741016",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Sampling imbalance classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122e5d2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.758090Z",
     "iopub.status.busy": "2023-04-06T04:50:11.757111Z",
     "iopub.status.idle": "2023-04-06T04:50:11.764391Z",
     "shell.execute_reply": "2023-04-06T04:50:11.763257Z"
    },
    "papermill": {
     "duration": 0.015755,
     "end_time": "2023-04-06T04:50:11.766745",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.750990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "def sampler(indices):\n",
    "    labels = [dataset.targets[x] for x in indices]\n",
    "    print(f'label length: {len(labels)}')\n",
    "    distribution = dict(Counter(labels))\n",
    "    class_weights = {k: 1/v for k, v in distribution.items()}\n",
    "\n",
    "    samples_weight = np.array([class_weights[t] for t in labels])\n",
    "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
    "    return sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ea8e283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.778262Z",
     "iopub.status.busy": "2023-04-06T04:50:11.777977Z",
     "iopub.status.idle": "2023-04-06T04:50:11.799361Z",
     "shell.execute_reply": "2023-04-06T04:50:11.798119Z"
    },
    "papermill": {
     "duration": 0.029752,
     "end_time": "2023-04-06T04:50:11.801538",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.771786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size: 4750\n",
      "label length: 3800\n",
      "label length: 950\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "from collections import Counter\n",
    "\n",
    "# Split validation data from training data\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "np.random.shuffle(indices) # shuffle the dataset before splitting into train and val\n",
    "print(f'dataset_size: {dataset_size}')\n",
    "\n",
    "split = int(np.floor(0.8 * dataset_size))\n",
    "train_indices, val_indices = indices[:split], indices[split:]\n",
    "\n",
    "# \n",
    "BATCH_SIZE = 24\n",
    "\n",
    "train = DataLoader(Subset(dataset, train_indices), sampler=sampler(train_indices), batch_size=BATCH_SIZE)\n",
    "val = DataLoader(Subset(dataset, val_indices), sampler=sampler(val_indices), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d801981",
   "metadata": {
    "papermill": {
     "duration": 0.004783,
     "end_time": "2023-04-06T04:50:11.812668",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.807885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualize distribution after sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b7c8c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.823647Z",
     "iopub.status.busy": "2023-04-06T04:50:11.823375Z",
     "iopub.status.idle": "2023-04-06T04:50:11.827624Z",
     "shell.execute_reply": "2023-04-06T04:50:11.826451Z"
    },
    "papermill": {
     "duration": 0.012359,
     "end_time": "2023-04-06T04:50:11.829953",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.817594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot class distribution histogram for training data\n",
    "# class_counts = [0]*len(dataset.classes)\n",
    "\n",
    "# for i, (_, label) in enumerate(train):\n",
    "#     for l in label:\n",
    "#         class_counts[l] += 1\n",
    "\n",
    "# # Plot class distribution histogram\n",
    "# plt.bar(dataset.classes, class_counts)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f2e8ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.841094Z",
     "iopub.status.busy": "2023-04-06T04:50:11.840834Z",
     "iopub.status.idle": "2023-04-06T04:50:11.845034Z",
     "shell.execute_reply": "2023-04-06T04:50:11.844044Z"
    },
    "papermill": {
     "duration": 0.01213,
     "end_time": "2023-04-06T04:50:11.847124",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.834994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Plot class distribution histogram for validation data\n",
    "# class_counts = [0]*len(dataset.classes)\n",
    "\n",
    "# for i, (_, label) in enumerate(val):\n",
    "#     for l in label:\n",
    "#         class_counts[l] += 1\n",
    "\n",
    "# # Plot class distribution histogram\n",
    "# plt.bar(dataset.classes, class_counts)\n",
    "# plt.xticks(rotation=90)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dd4791",
   "metadata": {
    "papermill": {
     "duration": 0.004765,
     "end_time": "2023-04-06T04:50:11.857171",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.852406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Visualize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85c6390d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.868665Z",
     "iopub.status.busy": "2023-04-06T04:50:11.868106Z",
     "iopub.status.idle": "2023-04-06T04:50:11.872942Z",
     "shell.execute_reply": "2023-04-06T04:50:11.872051Z"
    },
    "papermill": {
     "duration": 0.012917,
     "end_time": "2023-04-06T04:50:11.875017",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.862100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def visualizeBatch(batch, classes=None):\n",
    "#     # sample 8 indexes from BATCH_SIZE\n",
    "#     indexes = np.random.choice(BATCH_SIZE, 8, replace=False)\n",
    "#     for i, j in enumerate(indexes):\n",
    "#         image, idx = batch[0][j], batch[1][j]\n",
    "        \n",
    "#         ax = plt.subplot(2, 4, i + 1)\n",
    "#         image = image.cpu().numpy()\n",
    "#         image = image.transpose((1, 2, 0))\n",
    "#         image = (255.0 * image).astype('uint8')\n",
    "        \n",
    "#         plt.imshow(image)\n",
    "#         if classes is not None:\n",
    "#             plt.title(classes[idx])\n",
    "#         plt.axis('off')\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "09a0cc90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.886589Z",
     "iopub.status.busy": "2023-04-06T04:50:11.885899Z",
     "iopub.status.idle": "2023-04-06T04:50:11.890224Z",
     "shell.execute_reply": "2023-04-06T04:50:11.889344Z"
    },
    "papermill": {
     "duration": 0.012286,
     "end_time": "2023-04-06T04:50:11.892307",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.880021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainBatch = next(iter(train))\n",
    "# visualizeBatch(trainBatch, dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad60c974",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.903769Z",
     "iopub.status.busy": "2023-04-06T04:50:11.902907Z",
     "iopub.status.idle": "2023-04-06T04:50:11.907185Z",
     "shell.execute_reply": "2023-04-06T04:50:11.906315Z"
    },
    "papermill": {
     "duration": 0.011965,
     "end_time": "2023-04-06T04:50:11.909173",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.897208",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# testBatch = next(iter(test))\n",
    "# visualizeBatch(testBatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3984e03",
   "metadata": {
    "papermill": {
     "duration": 0.004772,
     "end_time": "2023-04-06T04:50:11.918981",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.914209",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### FineTuning resnet-50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6af45f86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:11.930630Z",
     "iopub.status.busy": "2023-04-06T04:50:11.929774Z",
     "iopub.status.idle": "2023-04-06T04:50:12.039223Z",
     "shell.execute_reply": "2023-04-06T04:50:12.038185Z"
    },
    "papermill": {
     "duration": 0.117736,
     "end_time": "2023-04-06T04:50:12.041789",
     "exception": false,
     "start_time": "2023-04-06T04:50:11.924053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \n",
    "                      'mps' if torch.backends.mps.is_built() else \n",
    "                      'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8488009",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:12.054029Z",
     "iopub.status.busy": "2023-04-06T04:50:12.053434Z",
     "iopub.status.idle": "2023-04-06T04:50:20.081045Z",
     "shell.execute_reply": "2023-04-06T04:50:20.079868Z"
    },
    "papermill": {
     "duration": 8.036611,
     "end_time": "2023-04-06T04:50:20.083720",
     "exception": false,
     "start_time": "2023-04-06T04:50:12.047109",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n",
      "- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ResNetModel, ResNetConfig\n",
    "from torch import nn\n",
    "from transformers.modeling_outputs import ImageClassifierOutputWithNoAttention\n",
    "\n",
    "# model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\").to(device)\n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, checkpoint=\"microsoft/resnet-50\", num_classes=12):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.model = ResNetModel.from_pretrained(checkpoint)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = torch.nn.Linear(2048, num_classes)\n",
    "    \n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.model(x)\n",
    "        x = self.pooling(x[0])\n",
    "        x = self.flatten(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.classifier(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
    "        \n",
    "        return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits)\n",
    "\n",
    "model = CustomResNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "963b59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "\n",
    "# # If true passed in sys argv, then load the model from checkpoint\n",
    "# if len(sys.argv) > 1 and sys.argv[1] == 'True':\n",
    "#     model.load_state_dict(torch.load('model.pth'))\n",
    "# model.load_state_dict(torch.load('best_model.pt', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bea17e48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:20.096708Z",
     "iopub.status.busy": "2023-04-06T04:50:20.096384Z",
     "iopub.status.idle": "2023-04-06T04:50:20.107239Z",
     "shell.execute_reply": "2023-04-06T04:50:20.106350Z"
    },
    "papermill": {
     "duration": 0.019721,
     "end_time": "2023-04-06T04:50:20.109344",
     "exception": false,
     "start_time": "2023-04-06T04:50:20.089623",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# define training loop\n",
    "def train_loop(model, train, val, optimizer, loss_fn, epochs=10):\n",
    "    best_val_acc = 0\n",
    "    pred_cm = torch.empty(0)\n",
    "    label_cm = torch.empty(0)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        loops = 0\n",
    "        for i, (image, label) in enumerate(tqdm(train)):\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(image, labels=label)\n",
    "            loss = loss_fn(output, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            loops += 1\n",
    "            predicted = output.logits.argmax(-1)\n",
    "            total_correct += (predicted == label).sum().item()\n",
    "        \n",
    "        print(f'Epoch: {epoch}, Training Loss: {total_loss/loops:.2f}, Training Accuracy: {(total_correct/(loops*BATCH_SIZE))*100:.2f}%')\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_correct = 0\n",
    "            loops = 0\n",
    "            for i, (image, label) in enumerate(tqdm(val)):\n",
    "                image = image.to(device)\n",
    "                label = label.to(device)\n",
    "                \n",
    "                output = model(image, labels=label)\n",
    "                loss = loss_fn(output, label)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                loops += 1\n",
    "                print(output.logits.shape)\n",
    "                predicted = output.logits.argmax(-1)\n",
    "                total_correct += (predicted == label).sum().item()\n",
    "                \n",
    "                # store predicted and label for confusion matrix\n",
    "                pred_cm = torch.cat((pred_cm, predicted.cpu()), 0)\n",
    "                label_cm = torch.cat((label_cm, label.cpu()), 0)\n",
    "                \n",
    "            print(f'Epoch: {epoch}, Validation Loss: {total_loss/loops:.2f}, Validation Accuracy: {(total_correct/(loops*BATCH_SIZE))*100:.2f}%')\n",
    "            \n",
    "            # Save model if validation accuracy is better than previous best\n",
    "            if (total_correct/(loops*BATCH_SIZE))*100 > best_val_acc:\n",
    "                best_val_acc = (total_correct/(loops*BATCH_SIZE))*100\n",
    "                torch.save(model.state_dict(), 'best_model.pt')\n",
    "                print(f'Best model saved with validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return model, pred_cm, label_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "309841fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNetForImageClassification(\n",
       "  (resnet): ResNetModel(\n",
       "    (embedder): ResNetEmbeddings(\n",
       "      (embedder): ResNetConvLayer(\n",
       "        (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (encoder): ResNetEncoder(\n",
       "      (stages): ModuleList(\n",
       "        (0): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (4): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (5): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ResNetForImageClassification\n",
    "\n",
    "ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "facbaa39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomResNet(\n",
       "  (model): ResNetModel(\n",
       "    (embedder): ResNetEmbeddings(\n",
       "      (embedder): ResNetConvLayer(\n",
       "        (convolution): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (activation): ReLU()\n",
       "      )\n",
       "      (pooler): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (encoder): ResNetEncoder(\n",
       "      (stages): ModuleList(\n",
       "        (0): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (3): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (4): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (5): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ResNetStage(\n",
       "          (layers): Sequential(\n",
       "            (0): ResNetBottleNeckLayer(\n",
       "              (shortcut): ResNetShortCut(\n",
       "                (convolution): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "                (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (1): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (2): ResNetBottleNeckLayer(\n",
       "              (shortcut): Identity()\n",
       "              (layer): Sequential(\n",
       "                (0): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (1): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): ReLU()\n",
       "                )\n",
       "                (2): ResNetConvLayer(\n",
       "                  (convolution): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "                  (normalization): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                  (activation): Identity()\n",
       "                )\n",
       "              )\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (classifier): Linear(in_features=2048, out_features=12, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c61c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:20.121144Z",
     "iopub.status.busy": "2023-04-06T04:50:20.120869Z",
     "iopub.status.idle": "2023-04-06T04:50:20.127377Z",
     "shell.execute_reply": "2023-04-06T04:50:20.126348Z"
    },
    "papermill": {
     "duration": 0.015007,
     "end_time": "2023-04-06T04:50:20.129571",
     "exception": false,
     "start_time": "2023-04-06T04:50:20.114564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "criteria = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "982d16c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T04:50:20.141279Z",
     "iopub.status.busy": "2023-04-06T04:50:20.141017Z",
     "iopub.status.idle": "2023-04-06T06:29:21.045084Z",
     "shell.execute_reply": "2023-04-06T06:29:21.043721Z"
    },
    "papermill": {
     "duration": 5940.91265,
     "end_time": "2023-04-06T06:29:21.047652",
     "exception": false,
     "start_time": "2023-04-06T04:50:20.135002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG:2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not ImageClassifierOutputWithNoAttention",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, pred_cm, label_cm \u001b[39m=\u001b[39m train_loop(model, train, val, optimizer, criteria, epochs\u001b[39m=\u001b[39;49mepoch)\n",
      "Cell \u001b[0;32mIn[23], line 21\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, train, val, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m output \u001b[39m=\u001b[39m model(image, labels\u001b[39m=\u001b[39mlabel)\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDEBUG:2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 21\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(output, label)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDEBUG:3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/torch/nn/functional.py:3049\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3047\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3048\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3049\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not ImageClassifierOutputWithNoAttention"
     ]
    }
   ],
   "source": [
    "model, pred_cm, label_cm = train_loop(model, train, val, optimizer, criteria, epochs=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "77b7b9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1900]), torch.Size([1900]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_cm.shape, label_cm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40721a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]), tensor([167, 145, 154, 170, 168, 150, 167, 158, 135, 148, 178, 160]))\n",
      "(tensor([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,  11.,\n",
      "        852.]), tensor([160, 141, 154, 163, 157, 159, 180, 161, 140, 150, 179, 155,   1]))\n"
     ]
    }
   ],
   "source": [
    "# all unique values and their count in label_cm\n",
    "print(torch.unique(label_cm, return_counts=True))\n",
    "print(torch.unique(pred_cm, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c5f2ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[144   0   0   1   2   0  12   0   0   0   0   1   0]\n",
      " [  0 141   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   1 152   0   1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 161   1   0   0   0   0   0   0   1   0]\n",
      " [  0   0   0   0 155   1   1   0   0   0   0   0   0]\n",
      " [  0   0   1   1   4 149   0   0   0   1   0   3   0]\n",
      " [ 22   0   1   1   2   0 153   0   0   0   0   1   0]\n",
      " [  1   0   0   0   0   0   0 158   0   0   0   2   0]\n",
      " [  0   0   0   2   1   0   0   0 135   2   0   0   0]\n",
      " [  0   2   0   3   0   0   0   0   0 145   0   0   0]\n",
      " [  0   1   0   1   0   0   0   0   0   0 177   0   0]\n",
      " [  0   0   0   0   2   0   1   0   0   0   0 152   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   1   0   0]]\n",
      "[ 90.         100.          98.7012987   98.77300613  98.72611465\n",
      "  93.71069182  85.          98.13664596  96.42857143  96.66666667\n",
      "  98.88268156  98.06451613   0.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat=confusion_matrix(pred_cm.numpy(), label_cm.numpy())\n",
    "print(conf_mat)\n",
    "\n",
    "# Per-class accuracy\n",
    "class_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\n",
    "print(class_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a596e671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 13)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7142a78b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-06T06:29:22.211039Z",
     "iopub.status.busy": "2023-04-06T06:29:22.210664Z",
     "iopub.status.idle": "2023-04-06T06:29:43.856877Z",
     "shell.execute_reply": "2023-04-06T06:29:43.854882Z"
    },
    "papermill": {
     "duration": 22.275706,
     "end_time": "2023-04-06T06:29:43.859041",
     "exception": false,
     "start_time": "2023-04-06T06:29:21.583335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/794 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     predicted \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlogits\u001b[39m.\u001b[39margmax(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m     \u001b[39m# append to dataframe\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mappend({\u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m: file_name\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39m'\u001b[39m\u001b[39mspecies\u001b[39m\u001b[39m'\u001b[39m: dataset\u001b[39m.\u001b[39mclasses[predicted]}, ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     26\u001b[0m \u001b[39m# Save file to csv\u001b[39;00m\n\u001b[1;32m     27\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39m../../working/submission.csv\u001b[39m\u001b[39m'\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/torch-gpu/lib/python3.8/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    # transforms.Resize((256, 256)),\n",
    "    transforms.Resize(size),\n",
    "    # RandomResizedCrop being used here --> https://huggingface.co/docs/transformers/main/en/tasks/image_classification\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n",
    "])\n",
    "\n",
    "# create empty dataframe\n",
    "df = pd.DataFrame(columns=['file', 'species'])\n",
    "\n",
    "# Run model over test data\n",
    "for file_name in tqdm(glob.glob(os.path.join('./test', '*.png'))):\n",
    "    image = transforms(Image.open(file_name)).to(device)\n",
    "    output = model(image.unsqueeze(0))\n",
    "    predicted = output.logits.argmax(-1).item()\n",
    "    \n",
    "    # append to dataframe\n",
    "    df = df.append({'file': file_name.split('/')[-1], 'species': dataset.classes[predicted]}, ignore_index=True)\n",
    "\n",
    "# Save file to csv\n",
    "df.to_csv('../../working/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5997.359479,
   "end_time": "2023-04-06T06:29:48.486914",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-06T04:49:51.127435",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0438b9e49a2d4660bf5ca1aaf6e527b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_477ee9a801f6481489598515ef013f40",
       "placeholder": "",
       "style": "IPY_MODEL_1176fc13a7e548beb0af022dedd7d04e",
       "value": "Downloading ()rocessor_config.json: 100%"
      }
     },
     "09292a1a500d43e5a983a42486b63c24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d7894424454b4b22a52d427358b61f05",
       "placeholder": "",
       "style": "IPY_MODEL_1f880f1cc0ba4253877db83124741b74",
       "value": " 103M/103M [00:01&lt;00:00, 50.4MB/s]"
      }
     },
     "1176fc13a7e548beb0af022dedd7d04e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "16eaa3bfa0b84cfab5d6a53d288bc313": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f880f1cc0ba4253877db83124741b74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "306fb4f1160743edbd266cc24f71a56b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "327192df9c6c4996b0205d6f5e42a652": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "337def99be6a480fbb132cff0b292d09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "406deb6615fd4e36b3e0ddb2860601e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7c8eb574b96e4446af8bc0a64f0fef09",
        "IPY_MODEL_ef3351511e1a4565a88dd4ecf59b6f34",
        "IPY_MODEL_a7b2fc98b732441a9d331e441cd30aab"
       ],
       "layout": "IPY_MODEL_327192df9c6c4996b0205d6f5e42a652"
      }
     },
     "4317d5ab6ac24a7b8c8400c6ce14d43d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "477ee9a801f6481489598515ef013f40": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a3b7cfdc3284405844fc365f2d6f79d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4bc3bd78ec0c43a49b1bfefb10cc4759": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5106ef0a35204860a009c5260beb5f85": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "57db9f24d9bd4102b8722a3218ed52ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16eaa3bfa0b84cfab5d6a53d288bc313",
       "max": 102567489,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_306fb4f1160743edbd266cc24f71a56b",
       "value": 102567489
      }
     },
     "7882013a679940079bcd22895a4b7aee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7c8eb574b96e4446af8bc0a64f0fef09": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_954665191b164539b0782a21748e786c",
       "placeholder": "",
       "style": "IPY_MODEL_cf9c0c69251948c48ea854591242fa87",
       "value": "Downloading ()lve/main/config.json: "
      }
     },
     "861c22ab51514b91b1f28c9512bf1d83": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8b0d88243c2d40b4b234f23c711cec74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "954665191b164539b0782a21748e786c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a5dfadc4693e4a0385ab63d6a3689217": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a77c007b1dc841f8b97322325c6d2db4",
       "placeholder": "",
       "style": "IPY_MODEL_da28359f22e0477d9d33a9a3bb985c0a",
       "value": " 266/266 [00:00&lt;00:00, 10.2kB/s]"
      }
     },
     "a77c007b1dc841f8b97322325c6d2db4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7b2fc98b732441a9d331e441cd30aab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4317d5ab6ac24a7b8c8400c6ce14d43d",
       "placeholder": "",
       "style": "IPY_MODEL_7882013a679940079bcd22895a4b7aee",
       "value": " 69.6k/? [00:00&lt;00:00, 3.84MB/s]"
      }
     },
     "c0cd7bd2fcf74972a73e3b82e2b75868": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c18ad88fc17f4f5c90b29c11782d4bb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e21b54367ad6426ca1ba978000d216f9",
        "IPY_MODEL_57db9f24d9bd4102b8722a3218ed52ce",
        "IPY_MODEL_09292a1a500d43e5a983a42486b63c24"
       ],
       "layout": "IPY_MODEL_8b0d88243c2d40b4b234f23c711cec74"
      }
     },
     "cf9c0c69251948c48ea854591242fa87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d7894424454b4b22a52d427358b61f05": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "da28359f22e0477d9d33a9a3bb985c0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "db4eb9cb316d4c479bf013b5b3ed49c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_861c22ab51514b91b1f28c9512bf1d83",
       "max": 266,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e1400d8808974135820d15017d70897d",
       "value": 266
      }
     },
     "e1400d8808974135820d15017d70897d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e21b54367ad6426ca1ba978000d216f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4a3b7cfdc3284405844fc365f2d6f79d",
       "placeholder": "",
       "style": "IPY_MODEL_337def99be6a480fbb132cff0b292d09",
       "value": "Downloading pytorch_model.bin: 100%"
      }
     },
     "e24655975e5e402898d1bb14f5dfdd68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0438b9e49a2d4660bf5ca1aaf6e527b9",
        "IPY_MODEL_db4eb9cb316d4c479bf013b5b3ed49c0",
        "IPY_MODEL_a5dfadc4693e4a0385ab63d6a3689217"
       ],
       "layout": "IPY_MODEL_c0cd7bd2fcf74972a73e3b82e2b75868"
      }
     },
     "ef3351511e1a4565a88dd4ecf59b6f34": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5106ef0a35204860a009c5260beb5f85",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4bc3bd78ec0c43a49b1bfefb10cc4759",
       "value": 1
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
