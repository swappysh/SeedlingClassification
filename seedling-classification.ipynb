{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Steps:\n\n- Load data and visualize a sample of them\n- Check for distribution across classes\n\n- Give ERM/IRM a shot to improve the performance","metadata":{}},{"cell_type":"code","source":"# Run following commands if running on local\n# !pip install kaggle\n\n# Download kaggle.json from kaggle website under profile->new API section\n# !kaggle competitions download -c plant-seedlings-classification\n# !unzip -q plant-seedlings-classification.zip","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:01:59.147232Z","iopub.execute_input":"2023-04-07T05:01:59.148055Z","iopub.status.idle":"2023-04-07T05:01:59.169839Z","shell.execute_reply.started":"2023-04-07T05:01:59.148003Z","shell.execute_reply":"2023-04-07T05:01:59.168706Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.chdir('/kaggle/input/plant-seedlings-classification/')","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:01:59.172463Z","iopub.execute_input":"2023-04-07T05:01:59.173280Z","iopub.status.idle":"2023-04-07T05:01:59.183473Z","shell.execute_reply.started":"2023-04-07T05:01:59.173227Z","shell.execute_reply":"2023-04-07T05:01:59.182560Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Folder structure\n# Training data\n# contains images in 12 folders, each folder contains images of a single class\n# Test data\n# contains all images in a single folder\n\n# Load the data\nfrom torchvision import datasets, transforms\nfrom transformers import AutoImageProcessor\n\nimage_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\nsize = (\n    (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n    if \"shortest_edge\" in image_processor.size\n    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n)\n\ntransforms = transforms.Compose([\n    # transforms.Resize((256, 256)),\n    transforms.Resize(size),\n    # RandomResizedCrop being used here --> https://huggingface.co/docs/transformers/main/en/tasks/image_classification\n    transforms.RandomRotation(360),\n    transforms.RandomResizedCrop(size),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n])\n\ndataset = datasets.ImageFolder('./train', transform=transforms)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:01:59.184603Z","iopub.execute_input":"2023-04-07T05:01:59.185184Z","iopub.status.idle":"2023-04-07T05:02:14.181371Z","shell.execute_reply.started":"2023-04-07T05:01:59.185149Z","shell.execute_reply":"2023-04-07T05:02:14.179859Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)rocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63f9f99017f441a0ae40dd4d4b7b43e7"}},"metadata":{}},{"name":"stderr","text":"Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Class Distribution","metadata":{"execution":{"iopub.status.busy":"2023-04-05T00:43:19.981148Z","iopub.execute_input":"2023-04-05T00:43:19.982073Z","iopub.status.idle":"2023-04-05T00:43:19.987338Z","shell.execute_reply.started":"2023-04-05T00:43:19.982023Z","shell.execute_reply":"2023-04-05T00:43:19.985893Z"}}},{"cell_type":"code","source":"# # Plot class distribution\n# from collections import Counter\n# import matplotlib.pyplot as plt\n\n# distribution = dict(Counter(dataset.targets))\n\n# # Plot class distribution histogram\n# plt.bar(list(map(lambda x: dataset.classes[x], distribution.keys())), distribution.values())\n# plt.xticks(rotation=90)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.183143Z","iopub.execute_input":"2023-04-07T05:02:14.184002Z","iopub.status.idle":"2023-04-07T05:02:14.189243Z","shell.execute_reply.started":"2023-04-07T05:02:14.183961Z","shell.execute_reply":"2023-04-07T05:02:14.187831Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Sampling imbalance classes","metadata":{"execution":{"iopub.status.busy":"2023-04-05T00:43:46.337206Z","iopub.execute_input":"2023-04-05T00:43:46.338478Z","iopub.status.idle":"2023-04-05T00:43:46.342784Z","shell.execute_reply.started":"2023-04-05T00:43:46.338426Z","shell.execute_reply":"2023-04-05T00:43:46.341565Z"}}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nimport numpy as np\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\ndef sampler(indices):\n    labels = [dataset.targets[x] for x in indices]\n    print(f'label length: {len(labels)}')\n    distribution = dict(Counter(labels))\n    class_weights = {k: 1/v for k, v in distribution.items()}\n\n    samples_weight = np.array([class_weights[t] for t in labels])\n    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n    return sampler","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.192863Z","iopub.execute_input":"2023-04-07T05:02:14.193891Z","iopub.status.idle":"2023-04-07T05:02:14.217568Z","shell.execute_reply.started":"2023-04-07T05:02:14.193852Z","shell.execute_reply":"2023-04-07T05:02:14.216426Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\nfrom torch.utils.data import Subset\nfrom collections import Counter\n\n# Split validation data from training data\ndataset_size = len(dataset)\nindices = list(range(dataset_size))\nnp.random.shuffle(indices) # shuffle the dataset before splitting into train and val\nprint(f'dataset_size: {dataset_size}')\n\nsplit = int(np.floor(0.8 * dataset_size))\ntrain_indices, val_indices = indices[:split], indices[split:]\n\n# \nBATCH_SIZE = 24\n\ntrain = DataLoader(Subset(dataset, train_indices), sampler=sampler(train_indices), batch_size=BATCH_SIZE)\nval = DataLoader(Subset(dataset, val_indices), sampler=sampler(val_indices), batch_size=BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.219113Z","iopub.execute_input":"2023-04-07T05:02:14.223263Z","iopub.status.idle":"2023-04-07T05:02:14.250612Z","shell.execute_reply.started":"2023-04-07T05:02:14.223195Z","shell.execute_reply":"2023-04-07T05:02:14.249312Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"dataset_size: 4750\nlabel length: 3800\nlabel length: 950\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Visualize distribution after sampling","metadata":{}},{"cell_type":"code","source":"# # Plot class distribution histogram for training data\n# class_counts = [0]*len(dataset.classes)\n\n# for i, (_, label) in enumerate(train):\n#     for l in label:\n#         class_counts[l] += 1\n\n# # Plot class distribution histogram\n# plt.bar(dataset.classes, class_counts)\n# plt.xticks(rotation=90)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.252061Z","iopub.execute_input":"2023-04-07T05:02:14.252782Z","iopub.status.idle":"2023-04-07T05:02:14.259022Z","shell.execute_reply.started":"2023-04-07T05:02:14.252733Z","shell.execute_reply":"2023-04-07T05:02:14.257587Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# # Plot class distribution histogram for validation data\n# class_counts = [0]*len(dataset.classes)\n\n# for i, (_, label) in enumerate(val):\n#     for l in label:\n#         class_counts[l] += 1\n\n# # Plot class distribution histogram\n# plt.bar(dataset.classes, class_counts)\n# plt.xticks(rotation=90)\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.260369Z","iopub.execute_input":"2023-04-07T05:02:14.260806Z","iopub.status.idle":"2023-04-07T05:02:14.269169Z","shell.execute_reply.started":"2023-04-07T05:02:14.260758Z","shell.execute_reply":"2023-04-07T05:02:14.268182Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Visualize images","metadata":{}},{"cell_type":"code","source":"# def visualizeBatch(batch, classes=None):\n#     # sample 8 indexes from BATCH_SIZE\n#     indexes = np.random.choice(BATCH_SIZE, 8, replace=False)\n#     for i, j in enumerate(indexes):\n#         image, idx = batch[0][j], batch[1][j]\n        \n#         ax = plt.subplot(2, 4, i + 1)\n#         image = image.cpu().numpy()\n#         image = image.transpose((1, 2, 0))\n#         image = (255.0 * image).astype('uint8')\n        \n#         plt.imshow(image)\n#         if classes is not None:\n#             plt.title(classes[idx])\n#         plt.axis('off')\n    \n#     plt.tight_layout()\n#     plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.272605Z","iopub.execute_input":"2023-04-07T05:02:14.273236Z","iopub.status.idle":"2023-04-07T05:02:14.280470Z","shell.execute_reply.started":"2023-04-07T05:02:14.273200Z","shell.execute_reply":"2023-04-07T05:02:14.279048Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# trainBatch = next(iter(train))\n# visualizeBatch(trainBatch, dataset.classes)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.281875Z","iopub.execute_input":"2023-04-07T05:02:14.282823Z","iopub.status.idle":"2023-04-07T05:02:14.293836Z","shell.execute_reply.started":"2023-04-07T05:02:14.282786Z","shell.execute_reply":"2023-04-07T05:02:14.292802Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# testBatch = next(iter(test))\n# visualizeBatch(testBatch)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.295355Z","iopub.execute_input":"2023-04-07T05:02:14.296423Z","iopub.status.idle":"2023-04-07T05:02:14.302486Z","shell.execute_reply.started":"2023-04-07T05:02:14.296384Z","shell.execute_reply":"2023-04-07T05:02:14.301470Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### FineTuning resnet-50","metadata":{}},{"cell_type":"code","source":"import torch\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else \n                      'mps' if torch.backends.mps.is_built() else \n                      'cpu')","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:14.304065Z","iopub.execute_input":"2023-04-07T05:02:14.304685Z","iopub.status.idle":"2023-04-07T05:02:14.312414Z","shell.execute_reply.started":"2023-04-07T05:02:14.304630Z","shell.execute_reply":"2023-04-07T05:02:14.311458Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from transformers import ResNetModel, ResNetConfig\nfrom torch import nn\nfrom transformers.modeling_outputs import ImageClassifierOutputWithNoAttention\n\n# model = ResNetForImageClassification.from_pretrained(\"microsoft/resnet-50\").to(device)\n\nclass CustomResNet(nn.Module):\n    def __init__(self, checkpoint=\"microsoft/resnet-50\", num_classes=12):\n        super(CustomResNet, self).__init__()\n        self.num_classes = num_classes\n        self.model = ResNetModel.from_pretrained(checkpoint)\n        self.flatten = nn.Flatten()\n        self.dropout = nn.Dropout(0.1)\n        self.pooling = nn.AdaptiveAvgPool2d((1, 1))\n        self.classifier = torch.nn.Linear(2048, num_classes)\n    \n    def forward(self, x, labels=None):\n        x = self.model(x)\n        x = self.pooling(x[0])\n        x = self.flatten(x)\n        x = self.dropout(x)\n        logits = self.classifier(x.view(-1, 2048))\n        \n        loss = None\n        if labels is not None:\n            loss_fct = torch.nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n        \n        return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits)\n\nmodel = CustomResNet().to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:31:13.321038Z","iopub.execute_input":"2023-04-07T05:31:13.321450Z","iopub.status.idle":"2023-04-07T05:31:13.857377Z","shell.execute_reply.started":"2023-04-07T05:31:13.321417Z","shell.execute_reply":"2023-04-07T05:31:13.856293Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at microsoft/resnet-50 were not used when initializing ResNetModel: ['classifier.1.weight', 'classifier.1.bias']\n- This IS expected if you are initializing ResNetModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing ResNetModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"# import sys\n\n# # If true passed in sys argv, then load the model from checkpoint\n# if len(sys.argv) > 1 and sys.argv[1] == 'True':\n#     model.load_state_dict(torch.load('best_model.pt'))\n# model.load_state_dict(torch.load('../seedling-best-model/best_model.pt', map_location=torch.device(device)))","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:31:14.843388Z","iopub.execute_input":"2023-04-07T05:31:14.844307Z","iopub.status.idle":"2023-04-07T05:31:14.849812Z","shell.execute_reply.started":"2023-04-07T05:31:14.844247Z","shell.execute_reply":"2023-04-07T05:31:14.848543Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# define trainingloop\ndef train_loop(model, train, val, optimizer, loss_fn, epochs=10):\n    pred_cm = torch.empty(0)\n    label_cm = torch.empty(0)\n    best_val_acc = 0\n    for epoch in range(epochs):\n        model.train()\n        total_loss = 0\n        total_correct = 0\n        loops = 0\n        for i, (image, label) in enumerate(tqdm(train)):\n            image = image.to(device)\n            label = label.to(device)\n            \n            optimizer.zero_grad()\n            output = model(image, labels=label)\n            loss = output.loss\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            loops += 1\n            predicted = output.logits.argmax(-1)\n            total_correct += (predicted == label).sum().item()\n        \n        print(f'Epoch: {epoch}, Training Loss: {total_loss/loops:.2f}, Training Accuracy: {(total_correct/(loops*BATCH_SIZE))*100:.2f}%')\n        \n        model.eval()\n        with torch.no_grad():\n            total_loss = 0\n            total_correct = 0\n            loops = 0\n            for i, (image, label) in enumerate(tqdm(val)):\n                image = image.to(device)\n                label = label.to(device)\n                \n                output = model(image, labels=label)\n                loss = output.loss\n                \n                total_loss += loss.item()\n                loops += 1\n                predicted = output.logits.argmax(-1)\n                total_correct += (predicted == label).sum().item()\n                \n                # store predicted and label for confusion matrix\n                pred_cm = torch.cat((pred_cm, predicted.cpu()), 0)\n                label_cm = torch.cat((label_cm, label.cpu()), 0)\n                \n            print(f'Epoch: {epoch}, Validation Loss: {total_loss/loops:.2f}, Validation Accuracy: {(total_correct/(loops*BATCH_SIZE))*100:.2f}%')\n            \n            # Save model if validation accuracy is better than previous best\n            if (total_correct/(loops*BATCH_SIZE))*100 > best_val_acc:\n                best_val_acc = (total_correct/(loops*BATCH_SIZE))*100\n                torch.save(model.state_dict(), 'best_model.pt')\n                print(f'Best model saved with validation accuracy: {best_val_acc:.2f}%')\n    \n    return model, pred_cm, label_cm","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:31:15.588034Z","iopub.execute_input":"2023-04-07T05:31:15.588467Z","iopub.status.idle":"2023-04-07T05:31:15.605290Z","shell.execute_reply.started":"2023-04-07T05:31:15.588430Z","shell.execute_reply":"2023-04-07T05:31:15.603889Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"epoch = 1\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\ncriteria = torch.nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:31:16.163969Z","iopub.execute_input":"2023-04-07T05:31:16.164592Z","iopub.status.idle":"2023-04-07T05:31:16.170640Z","shell.execute_reply.started":"2023-04-07T05:31:16.164554Z","shell.execute_reply":"2023-04-07T05:31:16.169377Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model, pred_cm, label_cm = train_loop(model, train, val, optimizer, criteria, epochs=epoch)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:31:16.822199Z","iopub.execute_input":"2023-04-07T05:31:16.822624Z","iopub.status.idle":"2023-04-07T05:34:04.402319Z","shell.execute_reply.started":"2023-04-07T05:31:16.822584Z","shell.execute_reply":"2023-04-07T05:34:04.400101Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stderr","text":" 13%|█▎        | 20/159 [02:47<19:24,  8.37s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/661376873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_cm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_cm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriteria\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/967038869.py\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, train, val, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/3537120616.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, labels)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/resnet/modeling_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         encoder_outputs = self.encoder(\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/resnet/modeling_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/resnet/modeling_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/resnet/modeling_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresidual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/resnet/modeling_resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mhidden_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 460\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Confusion matrix\nconf_mat=confusion_matrix(pred_cm.numpy(), label_cm.numpy())\nprint(conf_mat)\n\n# Per-class accuracy\nclass_accuracy=100*conf_mat.diagonal()/conf_mat.sum(1)\nprint(class_accuracy)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:18.410304Z","iopub.status.idle":"2023-04-07T05:02:18.410893Z","shell.execute_reply.started":"2023-04-07T05:02:18.410687Z","shell.execute_reply":"2023-04-07T05:02:18.410709Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, glob\nfrom PIL import Image\nimport pandas as pd\nfrom torchvision import transforms\n\ntransforms = transforms.Compose([\n    # transforms.Resize((256, 256)),\n    transforms.Resize(size),\n    # RandomResizedCrop being used here --> https://huggingface.co/docs/transformers/main/en/tasks/image_classification\n    transforms.ToTensor(),\n    transforms.Normalize(mean=image_processor.image_mean, std=image_processor.image_std),\n])\n\n# create empty dataframe\ndf = pd.DataFrame(columns=['file', 'species'])\n\n# Run model over test data\nfor file_name in tqdm(glob.glob(os.path.join('./test', '*.png'))):\n    image = transforms(Image.open(file_name)).to(device)\n    output = model(image.unsqueeze(0))\n    predicted = output.logits.argmax(-1).item()\n    \n    # append to dataframe\n    df = df.append({'file': file_name.split('/')[-1], 'species': dataset.classes[predicted]}, ignore_index=True)\n\n# Save file to csv\ndf.to_csv('../../working/submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-07T05:02:18.412020Z","iopub.status.idle":"2023-04-07T05:02:18.412619Z","shell.execute_reply.started":"2023-04-07T05:02:18.412416Z","shell.execute_reply":"2023-04-07T05:02:18.412438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}